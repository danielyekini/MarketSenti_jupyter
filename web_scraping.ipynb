{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Data Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pygooglenews import GoogleNews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = GoogleNews()\n",
    "search = gn.search('apple', from_ = '2024-02-01', to_ = '2024-02-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search['entries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the article data\n",
    "articles = []\n",
    "\n",
    "# Iterate over the entries and extract the required information\n",
    "for entry in search['entries']:\n",
    "    article = {\n",
    "        'title': entry['title'],\n",
    "        'link': entry['link'],\n",
    "        'published': entry['published'],\n",
    "        'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "    }\n",
    "    articles.append(article)\n",
    "\n",
    "# Convert the list of articles into a DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pygooglenews import GoogleNews\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "\n",
    "def collect_articles(query, output_file_path, start_date='2009-01-01', end_date='2009-01-02'):\n",
    "    # Initialize GoogleNews\n",
    "    gn = GoogleNews()\n",
    "\n",
    "    # Convert string dates to datetime objects\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Initialize an empty list to hold the article data\n",
    "    articles = []\n",
    "    total_articles = 0\n",
    "\n",
    "    # Calculate the total number of days for the progress bar\n",
    "    total_days = (end - start).days + 1  # +1 to include the end date\n",
    "\n",
    "    # Track the last month processed\n",
    "    last_date = start\n",
    "\n",
    "    # Iterate through each day in the specified date range with tqdm\n",
    "    for day in tqdm(range(total_days), desc=\"Collecting articles\"):\n",
    "        current_date = start + timedelta(days=day)\n",
    "\n",
    "        # Check if the month has changed or it's the last day\n",
    "        if current_date.month != last_date.month or current_date == end:\n",
    "            if articles:\n",
    "                # Convert the list of articles into a DataFrame and save\n",
    "                df = pd.DataFrame(articles)\n",
    "                file_exists = os.path.exists(output_file_path)\n",
    "                df.to_csv(output_file_path, mode='a', index=False, header=not file_exists)\n",
    "\n",
    "                # Update total articles count\n",
    "                total_articles += len(articles)\n",
    "\n",
    "                # Reset articles list for the new month\n",
    "                articles = []\n",
    "\n",
    "            last_date = current_date\n",
    "\n",
    "        # Format current_date for the search\n",
    "        search_date = current_date.strftime('%Y-%m-%d')\n",
    "        to_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "        # Perform the search for the current day\n",
    "        search = gn.search(query, from_=search_date, to_=to_date)\n",
    "\n",
    "        # Iterate over the entries and extract the required information\n",
    "        for entry in search['entries']:\n",
    "            article = {\n",
    "                'title': entry['title'],\n",
    "                'link': entry['link'],\n",
    "                'published': entry['published'],\n",
    "                'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "            }\n",
    "            articles.append(article)\n",
    "\n",
    "    # Return total articles saved\n",
    "    return total_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to remove non company related articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def filter_titles_by_keywords(df, inclusion_keywords, exclusion_keywords):\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Convert model to use IPEX optimized paths\n",
    "    model = model.to(ipex.DEVICE).eval()  # Use eval mode since we are doing inference\n",
    "\n",
    "    def get_embedding(text):\n",
    "        # Encode text and move tensors to the same device as model\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(ipex.DEVICE)\n",
    "        # Perform the forward pass without computing gradients (inference mode)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Precompute keyword embeddings and move them to the device\n",
    "    inclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in inclusion_keywords]\n",
    "    exclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in exclusion_keywords]\n",
    "\n",
    "    def is_related(title, positive_threshold=0.624, negative_threshold=0.9):\n",
    "        title_embedding = get_embedding(title).detach()\n",
    "        # Compute cosine similarities and check against thresholds\n",
    "        positive_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in inclusion_keyword_embeddings]\n",
    "        is_positive = any(similarity.item() > positive_threshold for similarity in positive_similarities)\n",
    "\n",
    "        negative_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in exclusion_keyword_embeddings]\n",
    "        is_negative = any(similarity.item() > negative_threshold for similarity in negative_similarities)\n",
    "\n",
    "        return is_positive and not is_negative\n",
    "\n",
    "    # Apply the is_related function to each title\n",
    "    df['is_related'] = [is_related(title) for title in tqdm(df['title'], desc=\"Filtering Titles By Keywords\")]\n",
    "\n",
    "    # Filter the DataFrame to keep only related entries and drop the 'is_related' column\n",
    "    related_df = df[df['is_related']].copy().drop(columns=['is_related'])\n",
    "\n",
    "    return related_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning/ Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def clean_dataset(file_path, inclusion_keywords, exclusion_keywords):\n",
    "    # Load the dataset\n",
    "    original_df = pd.read_csv(file_path)\n",
    "    df = original_df.copy()\n",
    "\n",
    "    # Extract the base name of the file without extension\n",
    "    base_name = os.path.basename(file_path).replace('.csv', '')\n",
    "\n",
    "    # Remove duplicated data\n",
    "    df.drop_duplicates(subset=['title', 'link', 'published'], inplace=True)\n",
    "\n",
    "    # Remove entries with null values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Remove entries with non-ASCII characters\n",
    "    df = df[df['title'].map(lambda x: x.isascii()) & df['source'].map(lambda x: x.isascii())]\n",
    "\n",
    "    # Remove non-company-related articles\n",
    "    df = filter_titles_by_keywords(df, inclusion_keywords, exclusion_keywords)\n",
    "\n",
    "    # Convert 'published' column to datetime to ensure proper sorting\n",
    "    df['published'] = pd.to_datetime(df['published'])\n",
    "\n",
    "    # Order the dataset by date\n",
    "    df.sort_values(by='published', inplace=True)\n",
    "\n",
    "    # Define cleaned and removed file paths according to the new directory structure\n",
    "    cleaned_file_path = os.path.join('data', 'cleaned', f'{base_name}_cleaned.csv')\n",
    "    removed_file_path = os.path.join('data', 'removed', f'{base_name}_removed.csv')\n",
    "\n",
    "    # Save the cleaned dataset back to a CSV file\n",
    "    df.to_csv(cleaned_file_path, index=False)\n",
    "    print(f\"Cleaned dataset saved to {cleaned_file_path}\")\n",
    "\n",
    "    # Identify all removed entries as the difference between the original and cleaned datasets\n",
    "    removed_entries = original_df[~original_df.index.isin(df.index)]\n",
    "\n",
    "    # Save the removed entries to a separate CSV file for inspection\n",
    "    removed_entries.to_csv(removed_file_path, index=False)\n",
    "    print(f\"Removed entries saved to {removed_file_path}\")\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_for(query, inclusion_keywords, exclusion_keywords):\n",
    "    # Path to raw dataset\n",
    "    file_path = f'./data/raw/{query}.csv' \n",
    "    # Collect the articles\n",
    "    num_data = collect_articles(query, file_path)\n",
    "    print(f\"Collected {num_data} articles and exported to '{file_path}'\")\n",
    "    # Clean the dataset\n",
    "    cleaned_df = clean_dataset(file_path, inclusion_keywords, exclusion_keywords)\n",
    "\n",
    "\n",
    "# # Specify your search query\n",
    "# query = 'apple'\n",
    "\n",
    "# # Keywords to compute keyword embeddings and compare with article tites\n",
    "# inclusion_keywords = []\n",
    "\n",
    "# exclusion_keywords = []\n",
    "\n",
    "# get_articles_for(query, inclusion_keywords, exclusion_keywords)\n",
    "\n",
    "# # Path to your dataset\n",
    "# file_path = './data/raw/apple.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles: 100%|██████████| 2/2 [00:04<00:00,  2.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 10 articles and exported to './data/raw/microsoft.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Titles By Keywords: 100%|██████████| 10/10 [00:00<00:00, 12.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset saved to data\\cleaned\\microsoft_cleaned.csv\n",
      "Removed entries saved to data\\removed\\microsoft_removed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "microsoft_inclusion_keywords = [\n",
    "    # Products and Hardware\n",
    "    \"Surface\", \"Surface Pro\", \"Surface Laptop\", \"Surface Book\",\n",
    "    \"Xbox\", \"Xbox One\", \"Xbox Series X\", \"Xbox Series S\",\n",
    "    \"Microsoft PC\", \"Microsoft HoloLens\", \"Windows Phone\",\n",
    "\n",
    "    # Software and Services\n",
    "    \"Windows updates\", \"Windows 10\", \"Windows 11\", \"Microsoft 365\",\n",
    "    \"Microsoft Office\", \"Outlook\", \"OneDrive\",\n",
    "    \"Microsoft Teams\", \"Skype\", \"Bing\",\n",
    "    \"Microsoft Edge\", \"Internet Explorer\",\n",
    "    \"Azure\", \"SQL Server\", \"Visual Studio\",\n",
    "\n",
    "    # Technology and Innovations\n",
    "    \"Cloud computing\", \"AI\", \"Machine Learning\",\n",
    "    \"Microsoft Cognitive Services\", \"Azure AI\",\n",
    "    \"Microsoft Security\", \"Windows Defender\",\n",
    "    \"GitHub\", \"Power Platform\", \"Dynamics 365\",\n",
    "\n",
    "    # Corporate and Culture\n",
    "    \"Satya Nadella\", \"Bill Gates\", \"Paul Allen\",\n",
    "    \"Microsoft Store\", \"Microsoft Research\",\n",
    "    \"Microsoft Campus\", \"Redmond\",\n",
    "    \"Microsoft Build\", \"Microsoft Ignite\",\n",
    "    \"Microsoft Philanthropies\", \"Code.org\",\n",
    "\n",
    "    # Events and Conferences\n",
    "    \"Microsoft Inspire\", \"Build Conference\",\n",
    "    \"Microsoft Developer Days\", \"TechEd\",\n",
    "    \"Xbox game releases\", \"Halo release\",\n",
    "\n",
    "    # Competitors and Industry\n",
    "    \"Microsoft vs. Apple\", \"Microsoft vs. Amazon\", \"Microsoft vs. Google\",\n",
    "    \"Microsoft's market share\", \"Cloud market\",\n",
    "    \"PC market\", \"Gaming market\", \"Enterprise solutions\",\n",
    "]\n",
    "\n",
    "microsoft_exclusion_keywords = [\n",
    "    # Unrelated \"windows\"\n",
    "    \"glass windows\", \"car windows\", \"house windows\", \"window cleaning\", \"window treatments\",\n",
    "    \n",
    "    # Unrelated \"surface\"\n",
    "    \"surface area\", \"surface water\", \"surface texture\", \"surface cleaning\", \"surface design\",\n",
    "\n",
    "    # Common phrases or idioms\n",
    "    \"opening a window of opportunity\", \"window into the soul\", \"surface-level examination\", \"only scratched the surface\",\n",
    "    \n",
    "    # Geographical references\n",
    "    \"Windows Valley\", \"Surface Creek\"\n",
    "]\n",
    "\n",
    "companies_keywords = {\n",
    "    'microsoft': {\n",
    "        'inclusion': microsoft_inclusion_keywords,\n",
    "        'exclusion': microsoft_exclusion_keywords\n",
    "    },\n",
    "    # Add more company queries with their keywords here\n",
    "}\n",
    "\n",
    "for company, keywords in companies_keywords.items():\n",
    "    get_articles_for(company, keywords['inclusion'], keywords['exclusion'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "inclusion_keywords = [\n",
    "    # Products and Hardware\n",
    "    \"iMac\", \"iMac Pro\",\n",
    "    \"Mac Mini\", \"Mac Pro\",\n",
    "    \"iPhone\", \"iPhone SE\", \"iPhone X\", \"iPhone 11\", \"iPhone 12\", \"iPhone 13\",\n",
    "    \"iPad Pro\", \"iPad Air\", \"iPad Mini\", \"iPadOS\",\n",
    "    \"Apple Watch Series\", \"watchOS\",\n",
    "    \"iPod\", \"iPod touch\", \"iPod Shuffle\"\n",
    "\n",
    "    # Software and Services\n",
    "    \"iOS updates\", \"macOS updates\", \"watchOS updates\",\n",
    "    \"iCloud\", \"iCloud Drive\", \"iCloud Photo Library\",\n",
    "    \"iTunes\", \"Apple Music\", \"Apple Podcasts\",\n",
    "    \"Apple TV+\", \"Apple TV app\", \"Apple Originals\",\n",
    "    \"Apple Arcade\", \"Game Center\", \"App Store\"\n",
    "\n",
    "    # Technology and Innovations\n",
    "    \"phones\", \"Retina display\", \"Liquid Retina\", \"Super Retina\",\n",
    "    \"Face ID\", \"Touch ID\", \"Apple Pay Cash\",\n",
    "    \"Apple M2 Chip\", \"A15 Bionic\", \"H1 Chip\",\n",
    "    \"TrueDepth camera\", \"Night mode\", \"Deep Fusion\",\n",
    "    \"LiDAR Scanner\", \"ARKit\",\n",
    "\n",
    "    # Corporate and Culture\n",
    "    \"Tim Cook\", \"Steve Jobs\", \"Steve Wozniak\", \"Jony Ive\",\n",
    "    \"Apple Store\", \"Genius Bar\", \"Today at Apple\",\n",
    "    \"Apple Campus 2\", \"Spaceship campus\",\n",
    "    \"Infinite Loop\", \"One Apple Park Way\",\n",
    "    \"Apple Design Awards\", \"Apple Entrepreneur Camp\",\n",
    "    \"AppleInsider\", \"MacRumors\"\n",
    "\n",
    "    # Events and Conferences\n",
    "    \"Special Events\", \"Apple Keynotes\",\n",
    "    \"Apple Spring Event\", \"Apple Fall Event\",\n",
    "    \"iPhone launch event\", \"iPad launch event\",\n",
    "    \"Apple Developer Forums\", \"Tech Talks\",\n",
    "    \"Apple Design Awards\", \"Shot on iPhone Challenge\",\n",
    "\n",
    "    # Competitors and Industry\n",
    "    \"Apple vs. Samsung\", \"Apple vs. Google\", \"Apple vs. Amazon\", \"Apple vs. Microsoft\",\n",
    "    \"iOS vs. Android\",\n",
    "    \"Mac vs. PC\",\n",
    "    \"Apple's market share\",\n",
    "    \"Innovations in consumer electronics\",\n",
    "    \"Trends in technology and mobile computing\",\n",
    "\n",
    "    # Potentially ambiguous headlines\n",
    "    \"Repurposing Your Dead Mac\",\n",
    "    \"Apple surprises us with a new, more-talkative iPod shuffle\",\n",
    "    \"Not Only Was Steve Jobs Sick, He Had A Liver Transplant\",\n",
    "    \"What's driving Steve Jobs?\",\n",
    "    \"A Suicide at an Apple Manufacturer in China\",\n",
    "    \"Compensation: $44000 And a MacBook - Cult of Mac\",\n",
    "    \"The iTunes App Store Rolls with the Travel Season\",\n",
    "    \"Steve Jobs Explains His Weight Loss in Healthnote\",\n",
    "    \"Apple iPod touch (3rd Generation) 32GB Review: iPod touch Review\",\n",
    "    \"He Put the Mac in Mackintosh\",\n",
    "    \"Apple's Iconic Ad\",\n",
    "    \"Apple's Latest Ad Takes Aim at Microsoft's 'Laptop Hunters' Campaign\"\n",
    "]\n",
    "\n",
    "exclusion_keywords = [\n",
    "    # Fruit and agriculture\n",
    "    \"apple orchard\", \"apple harvest\", \"apple picking\", \"apple variety\", \"apple cider\",\n",
    "    \"apple pie\", \"apple crumble\", \"apple sauce\", \"apple dessert\", \"apple recipe\",\n",
    "    \"apple nutrition\", \"apple health benefits\", \"apple tree\", \"apple seed\", \"apple farming\",\n",
    "\n",
    "    # Geographical references\n",
    "    \"Big Apple\", \"Apple Hill\", \"Apple City\", \"Apple River\"\n",
    "    \n",
    "    # Common phrases or idioms\n",
    "    \"apple of my eye\", \"bad apple\", \"apple doesn't fall far from the tree\", \"upset the apple cart\", \"compare apples to oranges\",\n",
    "    \"eating healthy\",\n",
    "    \n",
    "    # Unrelated products or services\n",
    "    \"apple shampoo\", \"apple cosmetics\", \"apple fragrance\", \"apple scented\", \"apple flavor\",\n",
    "    \"apple soda\", \"apple juice\", \"apple wine\", \"apple brandy\", \"apple beer\",\n",
    "]\n",
    "\n",
    "# Extend the existing exclusion keywords list with the new ones\n",
    "exclusion_keywords.extend([\n",
    "    \"Bikes With Non-EPA Tagged Exhausts Out of Big Apple\",\n",
    "    \"Copycat Fragrances\",\n",
    "    \"Swiss apple is key to Michelle Obama's youthful looks\",\n",
    "    \"Learn to grow tropical fruit at home in Houston\",\n",
    "    # Add any other previously identified exclusion keywords here...\n",
    "])\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_df = clean_dataset(file_path, inclusion_keywords, exclusion_keywords)\n",
    "cleaned_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
