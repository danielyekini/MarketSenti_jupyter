{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pygooglenews import GoogleNews\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_articles(query, output_file_path, start_date='2009-01-01', end_date='2024-02-20'):\n",
    "    # Initialize GoogleNews\n",
    "    gn = GoogleNews()\n",
    "\n",
    "    # Check if the directory of the output file exists, create it if not\n",
    "    output_dir = os.path.dirname(output_file_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert string dates to datetime objects\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Initialize an empty list to hold the article data\n",
    "    articles = []\n",
    "    total_entries = 0\n",
    "\n",
    "    # Calculate the total number of days for the progress bar\n",
    "    total_days = (end - start).days + 1  # +1 to include the end date\n",
    "\n",
    "    # Track the last month processed\n",
    "    last_date = start\n",
    "\n",
    "    # Iterate through each day in the specified date range with tqdm\n",
    "    for day in tqdm(range(total_days), desc=f\"Collecting articles from {start_date} to {end_date}\"):\n",
    "        try:\n",
    "            current_date = start + timedelta(days=day)\n",
    "\n",
    "            # Check if the month has changed or it's the last day\n",
    "            if current_date.month != last_date.month or current_date == end:\n",
    "                if articles:\n",
    "                    # Convert the list of articles into a DataFrame and save\n",
    "                    df = pd.DataFrame(articles)\n",
    "                    file_exists = os.path.exists(output_file_path)\n",
    "                    df.to_csv(output_file_path, mode='a', index=False, header=not file_exists)\n",
    "\n",
    "                    # Reset articles list for the new month\n",
    "                    articles = []\n",
    "\n",
    "                last_date = current_date\n",
    "\n",
    "            # Format current_date for the search\n",
    "            search_date = current_date.strftime('%Y-%m-%d')\n",
    "            to_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "            # Perform the search for the current day\n",
    "            search = gn.search(query, from_=search_date, to_=to_date)\n",
    "\n",
    "            # Iterate over the entries and extract the required information\n",
    "            for entry in search['entries']:\n",
    "                # Update total articles count\n",
    "                total_entries += 1\n",
    "                article = {\n",
    "                    'id': total_entries,\n",
    "                    'title': entry['title'],\n",
    "                    'link': entry['link'],\n",
    "                    'published': entry['published'],\n",
    "                    'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "                }\n",
    "                articles.append(article)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {current_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "\n",
    "        # Save any remaining articles\n",
    "    if articles:\n",
    "        df = pd.DataFrame(articles).drop_duplicates()\n",
    "        file_exists = os.path.exists(output_file_path)\n",
    "        df.to_csv(output_file_path, mode='a', index=False, header=not file_exists)\n",
    "\n",
    "    # Return total articles saved\n",
    "    return total_entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning/ Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, base_name, cleaned_or_removed, filter_status):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a CSV file with a structured naming convention.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be saved.\n",
    "    - base_name: The base name extracted from the original file path.\n",
    "    - cleaned_or_removed: Indicates whether the DataFrame contains cleaned or removed data.\n",
    "    - filter_status: Indicates the filtering status of the DataFrame ('unfiltered' or 'filtered').\n",
    "    \"\"\"\n",
    "    # Construct the directory path\n",
    "    dir_path = os.path.join(base_name, filter_status)\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_name = f\"{base_name}_{cleaned_or_removed}_{filter_status}.csv\"\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "    # Attempt to save the DataFrame, catching any exceptions\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"{cleaned_or_removed} dataset saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save {cleaned_or_removed} dataset to {file_path}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Articles By Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def filter_titles_by_keywords(df, inclusion_keywords, exclusion_keywords, exclusion_included):\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)  # Move model to GPU if available\n",
    "\n",
    "    def get_embedding(text):\n",
    "        # Encode text and move tensors to the same device as model\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        # Perform the forward pass and don't compute gradients as we're not training\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Precompute keyword embeddings and move them to the device\n",
    "    inclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in inclusion_keywords]\n",
    "    if (exclusion_included):\n",
    "        exclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in exclusion_keywords]\n",
    "\n",
    "    def is_related(title, positive_threshold=0.625, negative_threshold=0.73):\n",
    "        title_embedding = get_embedding(title).detach()\n",
    "        # Compute cosine similarities and check against thresholds\n",
    "        positive_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in inclusion_keyword_embeddings]\n",
    "        is_positive = any(similarity.item() > positive_threshold for similarity in positive_similarities)\n",
    "\n",
    "        if (exclusion_included):\n",
    "            negative_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in exclusion_keyword_embeddings]\n",
    "            is_negative = any(similarity.item() > negative_threshold for similarity in negative_similarities)\n",
    "            return is_positive and not is_negative\n",
    "\n",
    "        return is_positive\n",
    "\n",
    "    # Apply the is_related function to each title\n",
    "    df['is_related'] = [is_related(title) for title in tqdm(df['title'], desc=\"Filtering Titles By Keywords\")]\n",
    "\n",
    "    # Filter the DataFrame to keep only related entries\n",
    "    related_df = df[df['is_related']].copy().drop(columns=['is_related'])\n",
    "\n",
    "    return related_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(file_path, inclusion_keywords, exclusion_keywords, exclusion_included):\n",
    "    # Load the dataset\n",
    "    original_df = pd.read_csv(file_path)\n",
    "    unfiltered_df = original_df.copy()\n",
    "\n",
    "    # Extract the base name of the file without extension\n",
    "    base_name = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "     # Remove duplicated data\n",
    "    unfiltered_df.drop_duplicates(subset=['title', 'link', 'published'], inplace=True)\n",
    "\n",
    "    # Remove entries with null values\n",
    "    unfiltered_df.dropna(inplace=True)\n",
    "\n",
    "    for col in unfiltered_df.select_dtypes(include='object').columns:\n",
    "        unfiltered_df[col] = unfiltered_df[col].str.replace('â€™', '\\'')\n",
    "\n",
    "    # Remove entries with non-ASCII characters\n",
    "    unfiltered_df = unfiltered_df[unfiltered_df['title'].map(lambda x: x.isascii()) & unfiltered_df['source'].map(lambda x: x.isascii())]\n",
    "\n",
    "    # Convert 'published' column to datetime to ensure proper sorting\n",
    "    unfiltered_df['published'] = pd.to_datetime(unfiltered_df['published'])\n",
    "\n",
    "    # Order the dataset by date\n",
    "    unfiltered_df.sort_values(by='published', inplace=True)\n",
    "\n",
    "    # Order the dataset by date\n",
    "    unfiltered_df.sort_values(by='published', inplace=True)\n",
    "\n",
    "    # Save the cleaned dataset before keyword filtering\n",
    "    save_file(unfiltered_df, base_name, 'cleaned', 'unfiltered')\n",
    "\n",
    "    # Identify and save the removed dataset before keyword filtering\n",
    "    removed_unfiltered_df = original_df[~original_df.index.isin(unfiltered_df.index)]\n",
    "    save_file(removed_unfiltered_df, base_name, 'removed', 'unfiltered')\n",
    "\n",
    "    # Filter non-company-related articles\n",
    "    filtered_df = filter_titles_by_keywords(unfiltered_df, inclusion_keywords, exclusion_keywords, exclusion_included)\n",
    "\n",
    "    # Save the cleaned and removed datasets after further processing\n",
    "    save_file(filtered_df, base_name, 'cleaned', 'filtered')\n",
    "    removed_filtered_df = unfiltered_df[~unfiltered_df.index.isin(filtered_df.index)]\n",
    "    save_file(removed_filtered_df, base_name, 'removed', 'filtered')\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Articles For"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_for(query, inclusion_keywords, exclusion_keywords, exclusion_included, start_date='2009-01-01', end_date='2024-02-20'):\n",
    "    # Path to raw dataset\n",
    "    file_path = f'./{query}/{query}.csv' \n",
    "    # # Collect the articles\n",
    "    num_data = collect_articles(query, file_path, start_date, end_date)\n",
    "    print(f\"Collected {num_data} articles and exported to '{file_path}'\")\n",
    "    # Clean the dataset\n",
    "    clean_dataset(file_path, inclusion_keywords, exclusion_keywords, exclusion_included)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "boeing_inclusion_keywords = [\n",
    "    # Commercial Aircraft\n",
    "    \"Boeing 737\", \"Boeing 747\", \"Boeing 767\", \"Boeing 777\", \"Boeing 787 Dreamliner\",\n",
    "    \"Boeing MAX\", \"Boeing Business Jet\", \"Sky Interior\", \"Boeing Freighter\",\n",
    "    \n",
    "    # Defense, Space & Security\n",
    "    \"Boeing Defense\", \"Boeing Space\", \"Boeing Security\",\n",
    "    \"F/A-18 Super Hornet\", \"EA-18G Growler\", \"Boeing P-8 Poseidon\",\n",
    "    \"CH-47 Chinook\", \"AH-64 Apache\", \"V-22 Osprey\",\n",
    "    \"Boeing Satellites\", \"Boeing Missile Defense\", \"Boeing Autonomous Systems\",\n",
    "    \n",
    "    # Services\n",
    "    \"Boeing Global Services\", \"Boeing Support\", \"Boeing Training\",\n",
    "    \"Boeing Parts\", \"Boeing Analytics\", \"Boeing Digital Solutions\",\n",
    "    \n",
    "    # Technology and Innovations\n",
    "    \"Boeing Research\", \"Boeing Technology\", \"Boeing Innovations\",\n",
    "    \"Boeing Aerodynamics\", \"Boeing Materials Technology\",\n",
    "    \"Boeing Environmental Performance\", \"Boeing Fuel Efficiency\",\n",
    "    \n",
    "    # Corporate and Culture\n",
    "    \"Boeing CEO\", \"Boeing Leadership\", \"Boeing Culture\",\n",
    "    \"Boeing Stock\", \"Boeing Shareholders\", \"Boeing Financials\",\n",
    "    \"Boeing Ethics\", \"Boeing Careers\", \"Boeing History\",\n",
    "    \n",
    "    # Events and Conferences\n",
    "    \"Boeing Air Show\", \"Paris Air Show\", \"Farnborough Air Show\",\n",
    "    \"Boeing Earnings Call\", \"Boeing Investor Relations\",\n",
    "    \n",
    "    # Key Personnel and Locations\n",
    "    \"Dennis Muilenburg\", \"Dave Calhoun\", \"William Boeing\",\n",
    "    \"Boeing Chicago\", \"Boeing Seattle\", \"Boeing Everett\",\n",
    "    \n",
    "    # Notable Incidents and Developments\n",
    "    \"Boeing 737 MAX crisis\", \"Boeing groundings\", \"Boeing safety\",\n",
    "    \"Boeing return to service\", \"Boeing MCAS\", \"Boeing Starliner\",\n",
    "    \n",
    "    # Potentially ambiguous headlines\n",
    "    \"Boeing Is Likely to Overtake Airbus\",\n",
    "]\n",
    "\n",
    "boeing_exclusion_keywords = [\n",
    "    # Common names and unrelated \"boeing\"\n",
    "    \"Boeing family\", \"Boeing Field\", \"Boeing Creek\",\n",
    "    \n",
    "    # General aviation unrelated to The Boeing Company\n",
    "    \"Cessna\", \"Airbus\", \"Embraer\", \"Lockheed Martin\",\n",
    "    \"Private jet\", \"Airplane model\", \"Flight school\",\n",
    "    \n",
    "    # Common phrases or idioms\n",
    "    \"On a wing and a prayer\", \"Fly by the seat of one's pants\", \"When pigs fly\",\n",
    "    \"Jet set lifestyle\", \"Sky-high expectations\", \"Take under one's wing\",\n",
    "    \n",
    "    # Geographical references\n",
    "    \"Boeing Avenue\", \"Boeing Way\", \"Boeing Park\",\n",
    "    \n",
    "    # Completely unrelated contexts\n",
    "    \"Boeing boing sound\", \"Boeing out of control\", \"Boeing from work\",\n",
    "]\n",
    "\n",
    "blackrock_inclusion_keywords = [\n",
    "    # Financial Products and Services\n",
    "    \"BlackRock funds\", \"iShares\", \"ETFs\", \"BlackRock Mutual Funds\",\n",
    "    \"Aladdin\", \"BlackRock Solutions\", \"Risk management\", \"Asset management\",\n",
    "    \"BlackRock Alternative Investors\", \"Portfolio management\",\n",
    "    \"BlackRock Retirement Solutions\", \"BlackRock Liquidity\",\n",
    "    \n",
    "    # Investments and Strategies\n",
    "    \"Equity investments\", \"Fixed income\", \"Multi-asset solutions\",\n",
    "    \"Systematic investing\", \"Factor-based strategies\", \"Sustainable investing\",\n",
    "    \"Impact investing\", \"BlackRock Global Allocation Fund\",\n",
    "    \n",
    "    # Corporate Information and Culture\n",
    "    \"BlackRock CEO\", \"Larry Fink\", \"BlackRock leadership team\",\n",
    "    \"BlackRock Board of Directors\", \"BlackRock Annual Report\",\n",
    "    \"BlackRock ESG\", \"BlackRock corporate culture\",\n",
    "    \"BlackRock investment approach\", \"BlackRock careers\",\n",
    "    \n",
    "    # Technology and Innovations\n",
    "    \"BlackRock financial technology\", \"FinTech\", \"Robo-advisors\",\n",
    "    \"Digital wealth management\", \"Investment analytics\",\n",
    "    \n",
    "    # Key Personnel and Locations\n",
    "    \"Robert Kapito\",\"Barbara Novick\", \"Susan Wagner\", \n",
    "    \"Laurence D. Fink\", \"Robert S. Kapito\", \"Gary Shedlin\", \"Christopher J. Meade\", \"Mark Wiedman\",\n",
    "    \"BlackRock New York headquarters\",\n",
    "    \"BlackRock offices worldwide\",\n",
    "    \n",
    "    # Events and Conferences\n",
    "    \"BlackRock earnings call\", \"BlackRock investor day\",\n",
    "    \"BlackRock conference\",\n",
    "    \n",
    "    # Notable Developments and Initiatives\n",
    "    \"BlackRock's climate change strategy\", \"BlackRock's stake in companies\",\n",
    "    \"BlackRock shareholder activism\", \"BlackRock market outlook\",\n",
    "    \n",
    "    # Regulatory and Legal Aspects\n",
    "    \"BlackRock SEC filings\", \"BlackRock compliance\",\n",
    "    \"Financial regulations affecting BlackRock\",\n",
    "]\n",
    "\n",
    "blackrock_exclusion_keywords = [\n",
    "    # Geographical references and natural features\n",
    "    \"Black rock formations\", \"Black Rock City\", \"Black Rock Desert\",\n",
    "    \"Black rocks in geology\", \"Black Rock Forest\", \"Black Rock Beach\",\n",
    "    \n",
    "    # Common phrases or idioms\n",
    "    \"As black as a rock\", \"Between a rock and a hard place\",\n",
    "    \n",
    "    # Cultural references\n",
    "    \"Black Rock musical festival\", \"Black Rock art installation\",\n",
    "    \n",
    "    # Unrelated businesses and products\n",
    "    \"Black rock grill\", \"Black rock coffee\", \"Black rock landscaping\",\n",
    "    \"Black rock chic fashion\", \"Black Rock Bar & Grill\",\n",
    "    \n",
    "    # Miscellaneous\n",
    "    \"BlackRock mountain biking\", \"Black Rock sailing\",\n",
    "    \"Black rock candy\", \"Black rock mining\",\n",
    "]\n",
    "\n",
    "amazon_inclusion_keywords = [\n",
    "    # Online Retail and Services\n",
    "    \"Amazon Prime\", \"Amazon Marketplace\", \"Amazon Fresh\", \"Amazon Pantry\",\n",
    "    \"Amazon Fashion\", \"Amazon Basics\", \"Prime Day\", \"Kindle Store\",\n",
    "    \"Amazon Music\", \"Amazon Books\", \"Amazon Go\", \"Amazon Drive\",\n",
    "    \n",
    "    # Amazon Devices\n",
    "    \"Kindle\", \"Fire Tablet\", \"Fire TV\", \"Amazon Echo\", \"Alexa\",\n",
    "    \"Echo Dot\", \"Amazon Fire Stick\", \"Kindle Oasis\", \"Echo Show\",\n",
    "    \n",
    "    # Amazon Web Services (AWS)\n",
    "    \"AWS\", \"Amazon S3\", \"EC2\", \"AWS Lambda\", \"Amazon RDS\",\n",
    "    \"Amazon DynamoDB\", \"AWS CloudFront\", \"Amazon Aurora\",\n",
    "    \"AWS Marketplace\", \"Amazon CloudWatch\",\n",
    "    \n",
    "    # Technology and Innovations\n",
    "    \"Amazon Robotics\", \"Drone delivery\", \"Amazon AI\",\n",
    "    \"Amazon Machine Learning\", \"Amazon Lex\",\n",
    "    \n",
    "    # Corporate and Culture\n",
    "    \"Jeff Bezos\", \"Andy Jassy\", \"Amazon leadership\",\n",
    "    \"Amazon stock\", \"NASDAQ:AMZN\", \"Amazon annual report\",\n",
    "    \"Amazon sustainability\", \"Amazon career\",\n",
    "    \n",
    "    # Events and Conferences\n",
    "    \"re:Invent\", \"Amazon Web Services Summit\",\n",
    "    \n",
    "    # Key Personnel\n",
    "    \"Jeff Bezos\", \"Andy Jassy\", \"Werner Vogels\", \"Dave Clark\",\n",
    "    \"Jeff Blackburn\", \"Brian Olsavsky\", \"Douglas Herrington\",\n",
    "    \n",
    "    # Notable Developments\n",
    "    \"Amazon HQ2\", \"Amazon Air\", \"Amazon fulfillment center\",\n",
    "    \"Amazon second headquarters\", \"Amazon delivery network\",\n",
    "    \n",
    "    # Legal and Regulatory Issues\n",
    "    \"Amazon antitrust\", \"Amazon tax\", \"Amazon labor practices\",\n",
    "    \n",
    "    # Amazon Studios and Video\n",
    "    \"Amazon Original Series\", \"Amazon Studios\", \"Prime Video\",\n",
    "]\n",
    "\n",
    "amazon_exclusion_keywords = [\n",
    "    # Amazon Rainforest and River\n",
    "    \"Amazon rainforest\", \"Amazon river\", \"Amazon basin\",\n",
    "    \"Amazon deforestation\", \"Amazon conservation\",\n",
    "    \n",
    "    # Mythology and Ancient History\n",
    "    \"Amazon warriors\", \"Amazons\", \"Amazon mythology\",\n",
    "    \n",
    "    # Unrelated Businesses and Entities\n",
    "    \"Amazon fish\", \"Amazon parrot\", \"Amazon snakes\",\n",
    "    \n",
    "    # Common Phrases and Other Uses\n",
    "    \"Amazon-like\", \"Amazonian climate\", \"Amazon rainforest reviews\"\n",
    "    \n",
    "    # Geographical Locations\n",
    "    \"Amazon region\", \"Amazon state\", \"Hotel Amazon\",\n",
    "    \"Travel\", \"Trip\"\n",
    "    \n",
    "    # Ambiguous Phrases\n",
    "    \"Amazon of industry\", \"Amazon of the North\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Boeing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20:   7%|â–‹         | 396/5529 [57:02<1257:42:46, 882.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 2010-01-31: ('Connection aborted.', ConnectionResetError(10054, 'An existing connection was forcibly closed by the remote host', None, 10054, None))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5529/5529 [2:37:11<00:00,  1.71s/it]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 124174 articles and exported to './Boeing/Boeing.csv'\n",
      "cleaned dataset saved to Boeing\\unfiltered\\Boeing_cleaned_unfiltered.csv\n",
      "removed dataset saved to Boeing\\unfiltered\\Boeing_removed_unfiltered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Titles By Keywords: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 63822/63822 [1:19:31<00:00, 13.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset saved to Boeing\\filtered\\Boeing_cleaned_filtered.csv\n",
      "removed dataset saved to Boeing\\filtered\\Boeing_removed_filtered.csv\n",
      "Working on Blackrock...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5529/5529 [1:43:13<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 39619 articles and exported to './Blackrock/Blackrock.csv'\n",
      "cleaned dataset saved to Blackrock\\unfiltered\\Blackrock_cleaned_unfiltered.csv\n",
      "removed dataset saved to Blackrock\\unfiltered\\Blackrock_removed_unfiltered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Titles By Keywords: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20046/20046 [25:27<00:00, 13.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset saved to Blackrock\\filtered\\Blackrock_cleaned_filtered.csv\n",
      "removed dataset saved to Blackrock\\filtered\\Blackrock_removed_filtered.csv\n",
      "Working on Amazon...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5529/5529 [1:50:59<00:00,  1.20s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 214441 articles and exported to './Amazon/Amazon.csv'\n",
      "cleaned dataset saved to Amazon\\unfiltered\\Amazon_cleaned_unfiltered.csv\n",
      "removed dataset saved to Amazon\\unfiltered\\Amazon_removed_unfiltered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Titles By Keywords: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 107780/107780 [2:13:46<00:00, 13.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset saved to Amazon\\filtered\\Amazon_cleaned_filtered.csv\n",
      "removed dataset saved to Amazon\\filtered\\Amazon_removed_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "\n",
    "start_date='2013-01-01'\n",
    "end_date='2013-01-05'\n",
    "\n",
    "companies_keywords = {\n",
    "    'Boeing': {\n",
    "        'inclusion': boeing_inclusion_keywords,\n",
    "        'exclusion': boeing_exclusion_keywords,\n",
    "        'exclusion_included': True,\n",
    "    },\n",
    "    'Blackrock': {\n",
    "        'inclusion': blackrock_inclusion_keywords,\n",
    "        'exclusion': blackrock_exclusion_keywords,\n",
    "        'exclusion_included': True,\n",
    "    },\n",
    "    'Amazon': {\n",
    "        'inclusion': amazon_inclusion_keywords,\n",
    "        'exclusion': amazon_exclusion_keywords,\n",
    "        'exclusion_included': True,\n",
    "    },\n",
    "    # Add more company queries with their keywords here\n",
    "}\n",
    "\n",
    "for company, keywords in companies_keywords.items():\n",
    "    print(f\"Working on {company}...\")\n",
    "    get_articles_for(company, keywords['inclusion'], keywords['exclusion'], keywords['exclusion_included'], )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "apple_inclusion_keywords = [\n",
    "    # Products and Hardware\n",
    "    \"iMac\", \"iMac Pro\",\n",
    "    \"Mac Mini\", \"Mac Pro\",\n",
    "    \"iPhone\", \"iPhone SE\", \"iPhone X\", \"iPhone 11\", \"iPhone 12\", \"iPhone 13\",\n",
    "    \"iPad Pro\", \"iPad Air\", \"iPad Mini\", \"iPadOS\",\n",
    "    \"Apple Watch Series\", \"watchOS\",\n",
    "    \"iPod\", \"iPod touch\", \"iPod Shuffle\",\n",
    "\n",
    "    # Software and Services\n",
    "    \"iOS updates\", \"macOS updates\", \"watchOS updates\",\n",
    "    \"iCloud\", \"iCloud Drive\", \"iCloud Photo Library\",\n",
    "    \"iTunes\", \"Apple Music\", \"Apple Podcasts\",\n",
    "    \"Apple TV+\", \"Apple TV app\", \"Apple Originals\",\n",
    "    \"Apple Arcade\", \"Game Center\", \"App Store\",\n",
    "\n",
    "    # Technology and Innovations\n",
    "    \"phones\", \"Retina display\", \"Liquid Retina\", \"Super Retina\",\n",
    "    \"Face ID\", \"Touch ID\", \"Apple Pay Cash\",\n",
    "    \"Apple M2 Chip\", \"A15 Bionic\", \"H1 Chip\",\n",
    "    \"TrueDepth camera\", \"Night mode\", \"Deep Fusion\",\n",
    "    \"LiDAR Scanner\", \"ARKit\",\n",
    "\n",
    "    # Corporate and Culture\n",
    "    \"Tim Cook\", \"Steve Jobs\", \"Steve Wozniak\", \"Jony Ive\",\n",
    "    \"Apple Store\", \"Genius Bar\", \"Today at Apple\",\n",
    "    \"Apple Campus 2\", \"Spaceship campus\",\n",
    "    \"Infinite Loop\", \"One Apple Park Way\",\n",
    "    \"Apple Design Awards\", \"Apple Entrepreneur Camp\",\n",
    "    \"AppleInsider\", \"MacRumors\",\n",
    "\n",
    "    # Events and Conferences\n",
    "    \"Special Events\", \"Apple Keynotes\",\n",
    "    \"Apple Spring Event\", \"Apple Fall Event\",\n",
    "    \"iPhone launch event\", \"iPad launch event\",\n",
    "    \"Apple Developer Forums\", \"Tech Talks\",\n",
    "    \"Apple Design Awards\", \"Shot on iPhone Challenge\",\n",
    "\n",
    "    # Competitors and Industry\n",
    "    \"Apple vs. Samsung\", \"Apple vs. Google\", \"Apple vs. Amazon\", \"Apple vs. Microsoft\",\n",
    "    \"iOS vs. Android\",\n",
    "    \"Mac vs. PC\",\n",
    "    \"Apple's market share\",\n",
    "    \"Innovations in consumer electronics\",\n",
    "    \"Trends in technology and mobile computing\",\n",
    "\n",
    "    # Potentially ambiguous headlines\n",
    "    \"Repurposing Your Dead Mac\",\n",
    "    \"Apple surprises us with a new, more-talkative iPod shuffle\",\n",
    "    \"Not Only Was Steve Jobs Sick, He Had A Liver Transplant\",\n",
    "    \"What's driving Steve Jobs?\",\n",
    "    \"A Suicide at an Apple Manufacturer in China\",\n",
    "    \"Compensation: $44000 And a MacBook - Cult of Mac\",\n",
    "    \"The iTunes App Store Rolls with the Travel Season\",\n",
    "    \"Steve Jobs Explains His Weight Loss in Healthnote\",\n",
    "    \"Apple iPod touch (3rd Generation) 32GB Review: iPod touch Review\",\n",
    "    \"He Put the Mac in Mackintosh\",\n",
    "    \"Apple's Iconic Ad\",\n",
    "    \"Apple's Latest Ad Takes Aim at Microsoft's 'Laptop Hunters' Campaign\",\n",
    "]\n",
    "\n",
    "apple_exclusion_keywords = [\n",
    "    # Fruit and agriculture\n",
    "    \"apple orchard\", \"apple harvest\", \"apple picking\", \"apple variety\", \"apple cider\", \"apples\",\n",
    "    \"apple pie\", \"apple crumble\", \"apple sauce\", \"apple dessert\", \"apple recipe\",\n",
    "    \"apple nutrition\", \"apple health benefits\", \"apple tree\", \"apple seed\", \"apple farming\",\n",
    "\n",
    "    # Geographical references\n",
    "    \"Big Apple\", \"Apple Hill\", \"Apple City\", \"Apple River\", \"Apple Blossom Festival\"\n",
    "    \n",
    "    # Common phrases or idioms\n",
    "    \"apple of my eye\", \"bad apple\", \"apple doesn't fall far from the tree\", \"upset the apple cart\", \"compare apples to oranges\",\n",
    "    \"eating healthy\", \n",
    "    \n",
    "    # Unrelated products or services\n",
    "    \"apple shampoo\", \"apple cosmetics\", \"apple fragrance\", \"apple scented\", \"apple flavor\",\n",
    "    \"apple soda\", \"apple juice\", \"apple wine\", \"apple brandy\", \"apple beer\",\n",
    "]\n",
    "\n",
    "# Extend the existing exclusion keywords list with the new ones\n",
    "apple_exclusion_keywords.extend([\n",
    "    \"Bikes With Non-EPA Tagged Exhausts Out of Big Apple\",\n",
    "    \"Copycat Fragrances\",\n",
    "    \"Swiss apple is key to Michelle Obama's youthful looks\",\n",
    "    \"Learn to grow tropical fruit at home in Houston\",\n",
    "    # Add any other previously identified exclusion keywords here...\n",
    "])\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_unfiltered_df = clean_dataset(file_path, inclusion_keywords, exclusion_keywords)\n",
    "cleaned_unfiltered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gn = GoogleNews()\n",
    "# search = gn.search('apple', from_ = '2024-02-01', to_ = '2024-02-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to hold the article data\n",
    "# articles = []\n",
    "\n",
    "# # Iterate over the entries and extract the required information\n",
    "# for entry in search['entries']:\n",
    "#     article = {\n",
    "#         'title': entry['title'],\n",
    "#         'link': entry['link'],\n",
    "#         'published': entry['published'],\n",
    "#         'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "#     }\n",
    "#     articles.append(article)\n",
    "\n",
    "# # Convert the list of articles into a DataFrame\n",
    "# df = pd.DataFrame(articles)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = f'./Apple/apple.csv' \n",
    "\n",
    "# year = 2018\n",
    "\n",
    "# month = 4\n",
    "\n",
    "# # Load the CSV file into a DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Convert the date column to datetime format\n",
    "# # Replace 'published' with the name of your date column\n",
    "# df['published'] = pd.to_datetime(df['published'])\n",
    "\n",
    "# # Filter the DataFrame for the desired year\n",
    "# # Replace 2020 with the year you want to extract\n",
    "# df_year = df[(df['published'].dt.year == year) & (df['published'].dt.month == month)]\n",
    "\n",
    "# # Save the filtered DataFrame to a new CSV file\n",
    "# df_year.to_csv(f'./test_data/raw/apple_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_datasets(file_path1, file_path2, output_file_path):\n",
    "#     # Load the datasets from the given file paths\n",
    "#     df1 = pd.read_csv(file_path1)\n",
    "#     df2 = pd.read_csv(file_path2)\n",
    "    \n",
    "#     # Assuming 'id' is the column name for IDs in your datasets\n",
    "#     # Adjust IDs in df2 to continue from the last ID in df1\n",
    "#     last_id_df1 = df1['id'].max()\n",
    "#     df2['id'] = df2['id'].apply(lambda x: x + last_id_df1)\n",
    "\n",
    "#     # Merge the datasets without resetting the index\n",
    "#     merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "#     # Save the resulting dataset to the specified file\n",
    "#     merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "#     print(f\"Merged dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_dates(file_path, date_to_remove):\n",
    "\n",
    "#     df = pd.read_csv(file_path)\n",
    "\n",
    "#     df['published'] = pd.to_datetime(df['published'])\n",
    "\n",
    "#     # Convert the string to a datetime object for comparison\n",
    "#     date_to_remove = pd.to_datetime(date_to_remove)\n",
    "\n",
    "#     # Filter out the entries for the specified date\n",
    "#     df_filtered = df[df['published'].dt.date != date_to_remove.date()]\n",
    "\n",
    "#     # Save the resulting dataset to the specified file\n",
    "#     df_filtered.to_csv(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
