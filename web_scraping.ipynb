{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pygooglenews import GoogleNews\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_articles(query, output_file_path, start_date='2015-09-05', end_date='2015-09-06'):\n",
    "    # Initialize GoogleNews\n",
    "    gn = GoogleNews()\n",
    "\n",
    "    # Convert string dates to datetime objects\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Initialize an empty list to hold the article data\n",
    "    articles = []\n",
    "    total_articles = 0\n",
    "\n",
    "    # Calculate the total number of days for the progress bar\n",
    "    total_days = (end - start).days + 1  # +1 to include the end date\n",
    "\n",
    "    # Track the last month processed\n",
    "    last_date = start\n",
    "\n",
    "    # Iterate through each day in the specified date range with tqdm\n",
    "    for day in tqdm(range(total_days), desc=\"Collecting articles\"):\n",
    "        try:\n",
    "            current_date = start + timedelta(days=day)\n",
    "\n",
    "            # Check if the month has changed or it's the last day\n",
    "            if current_date.month != last_date.month or current_date == end:\n",
    "                if articles:\n",
    "                    # Convert the list of articles into a DataFrame and save\n",
    "                    df = pd.DataFrame(articles)\n",
    "                    file_exists = os.path.exists(output_file_path)\n",
    "                    df.to_csv(output_file_path, mode='a', index=False, header=not file_exists)\n",
    "\n",
    "                    # Update total articles count\n",
    "                    total_articles += len(articles)\n",
    "\n",
    "                    # Reset articles list for the new month\n",
    "                    articles = []\n",
    "\n",
    "                last_date = current_date\n",
    "\n",
    "            # Format current_date for the search\n",
    "            search_date = current_date.strftime('%Y-%m-%d')\n",
    "            to_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "            # Perform the search for the current day\n",
    "            search = gn.search(query, from_=search_date, to_=to_date)\n",
    "\n",
    "            # Iterate over the entries and extract the required information\n",
    "            for entry in search['entries']:\n",
    "                article = {\n",
    "                    'title': entry['title'],\n",
    "                    'link': entry['link'],\n",
    "                    'published': entry['published'],\n",
    "                    'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "                }\n",
    "                articles.append(article)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {current_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "\n",
    "    # Return total articles saved\n",
    "    return total_articles\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning/ Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, base_name, cleaned_or_removed, filter_status):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a CSV file with a structured naming convention.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be saved.\n",
    "    - base_name: The base name extracted from the original file path.\n",
    "    - cleaned_or_removed: Indicates whether the DataFrame contains cleaned or removed data.\n",
    "    - filter_status: Indicates the filtering status of the DataFrame ('unfiltered' or 'filtered').\n",
    "    \"\"\"\n",
    "    # Construct the directory path\n",
    "    dir_path = os.path.join('data', cleaned_or_removed, filter_status)\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_name = f\"{base_name}_{cleaned_or_removed}_{filter_status}.csv\"\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "    # Attempt to save the DataFrame, catching any exceptions\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"{cleaned_or_removed} dataset saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save {cleaned_or_removed} dataset to {file_path}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Articles By Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def filter_titles_by_keywords(df, inclusion_keywords, exclusion_keywords):\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)  # Move model to GPU if available\n",
    "\n",
    "    def get_embedding(text):\n",
    "        # Encode text and move tensors to the same device as model\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        # Perform the forward pass and don't compute gradients as we're not training\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Precompute keyword embeddings and move them to the device\n",
    "    inclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in inclusion_keywords]\n",
    "    exclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in exclusion_keywords]\n",
    "\n",
    "    def is_related(title, positive_threshold=0.624, negative_threshold=0.9):\n",
    "        title_embedding = get_embedding(title).detach()\n",
    "        # Compute cosine similarities and check against thresholds\n",
    "        positive_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in inclusion_keyword_embeddings]\n",
    "        is_positive = any(similarity.item() > positive_threshold for similarity in positive_similarities)\n",
    "\n",
    "        negative_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in exclusion_keyword_embeddings]\n",
    "        is_negative = any(similarity.item() > negative_threshold for similarity in negative_similarities)\n",
    "\n",
    "        return is_positive and not is_negative\n",
    "\n",
    "    # Apply the is_related function to each title\n",
    "    df['is_related'] = [is_related(title) for title in tqdm(df['title'], desc=\"Filtering Titles By Keywords\")]\n",
    "\n",
    "    # Filter the DataFrame to keep only related entries and drop the 'is_related' column\n",
    "    related_df = df[df['is_related']].copy().drop(columns=['is_related'])\n",
    "\n",
    "    return related_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(file_path, inclusion_keywords, exclusion_keywords):\n",
    "    # Load the dataset\n",
    "    original_df = pd.read_csv(file_path)\n",
    "    unfiltered_df = original_df.copy()\n",
    "\n",
    "    # Extract the base name of the file without extension\n",
    "    base_name = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "     # Remove duplicated data\n",
    "    unfiltered_df.drop_duplicates(subset=['title', 'link', 'published'], inplace=True)\n",
    "\n",
    "    # Remove entries with null values\n",
    "    unfiltered_df.dropna(inplace=True)\n",
    "\n",
    "    # Remove entries with non-ASCII characters\n",
    "    unfiltered_df = unfiltered_df[unfiltered_df['title'].map(lambda x: x.isascii()) & unfiltered_df['source'].map(lambda x: x.isascii())]\n",
    "\n",
    "    # Convert 'published' column to datetime to ensure proper sorting\n",
    "    unfiltered_df['published'] = pd.to_datetime(unfiltered_df['published'])\n",
    "\n",
    "    # Order the dataset by date\n",
    "    unfiltered_df.sort_values(by='published', inplace=True)\n",
    "\n",
    "    # Order the dataset by date\n",
    "    unfiltered_df.sort_values(by='published', inplace=True)\n",
    "\n",
    "    # Save the cleaned dataset before keyword filtering\n",
    "    save_file(unfiltered_df, base_name, 'cleaned', 'unfiltered')\n",
    "\n",
    "    # Identify and save the removed dataset before keyword filtering\n",
    "    removed_unfiltered_df = original_df[~original_df.index.isin(unfiltered_df.index)]\n",
    "    save_file(removed_unfiltered_df, base_name, 'removed', 'unfiltered')\n",
    "\n",
    "    # Filter non-company-related articles\n",
    "    filtered_df = filter_titles_by_keywords(unfiltered_df, inclusion_keywords, exclusion_keywords)\n",
    "\n",
    "    # Save the cleaned and removed datasets after further processing\n",
    "    save_file(filtered_df, base_name, 'cleaned', 'filtered')\n",
    "    removed_filtered_df = unfiltered_df[~unfiltered_df.index.isin(filtered_df.index)]\n",
    "    save_file(removed_filtered_df, base_name, 'removed', 'filtered')\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Articles For"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_for(query, inclusion_keywords, exclusion_keywords):\n",
    "    # Path to raw dataset\n",
    "    file_path = f'./data/raw/{query}.csv' \n",
    "    # # Collect the articles\n",
    "    # num_data = collect_articles(query, file_path)\n",
    "    # print(f\"Collected {num_data} articles and exported to '{file_path}'\")\n",
    "    # Clean the dataset\n",
    "    clean_dataset(file_path, inclusion_keywords, exclusion_keywords)\n",
    "\n",
    "\n",
    "# # Specify your search query\n",
    "# query = 'apple'\n",
    "\n",
    "# # Keywords to compute keyword embeddings and compare with article tites\n",
    "# inclusion_keywords = []\n",
    "\n",
    "# exclusion_keywords = []\n",
    "\n",
    "# get_articles_for(query, inclusion_keywords, exclusion_keywords)\n",
    "\n",
    "# # Path to your dataset\n",
    "# file_path = './data/raw/apple.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset saved to data\\cleaned\\unfiltered\\microsoft_cleaned_unfiltered.csv\n",
      "removed dataset saved to data\\removed\\unfiltered\\microsoft_removed_unfiltered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Titles By Keywords: 100%|██████████| 96774/96774 [2:20:05<00:00, 11.51it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset saved to data\\cleaned\\filtered\\microsoft_cleaned_filtered.csv\n",
      "removed dataset saved to data\\removed\\filtered\\microsoft_removed_filtered.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "microsoft_inclusion_keywords = [\n",
    "    # Products and Hardware\n",
    "    \"Surface\", \"Surface Pro\", \"Surface Laptop\", \"Surface Book\",\n",
    "    \"Xbox\", \"Xbox One\", \"Xbox Series X\", \"Xbox Series S\",\n",
    "    \"Microsoft PC\", \"Microsoft HoloLens\", \"Windows Phone\",\n",
    "\n",
    "    # Software and Services\n",
    "    \"Windows updates\", \"Windows 10\", \"Windows 11\", \"Microsoft 365\",\n",
    "    \"Microsoft Office\", \"Outlook\", \"OneDrive\",\n",
    "    \"Microsoft Teams\", \"Skype\", \"Bing\",\n",
    "    \"Microsoft Edge\", \"Internet Explorer\",\n",
    "    \"Azure\", \"SQL Server\", \"Visual Studio\",\n",
    "\n",
    "    # Technology and Innovations\n",
    "    \"Cloud computing\", \"AI\", \"Machine Learning\",\n",
    "    \"Microsoft Cognitive Services\", \"Azure AI\",\n",
    "    \"Microsoft Security\", \"Windows Defender\",\n",
    "    \"GitHub\", \"Power Platform\", \"Dynamics 365\",\n",
    "\n",
    "    # Corporate and Culture\n",
    "    \"Satya Nadella\", \"Bill Gates\", \"Paul Allen\",\n",
    "    \"Microsoft Store\", \"Microsoft Research\",\n",
    "    \"Microsoft Campus\", \"Redmond\",\n",
    "    \"Microsoft Build\", \"Microsoft Ignite\",\n",
    "    \"Microsoft Philanthropies\", \"Code.org\",\n",
    "\n",
    "    # Events and Conferences\n",
    "    \"Microsoft Inspire\", \"Build Conference\",\n",
    "    \"Microsoft Developer Days\", \"TechEd\",\n",
    "    \"Xbox game releases\", \"Halo release\",\n",
    "\n",
    "    # Competitors and Industry\n",
    "    \"Microsoft vs. Apple\", \"Microsoft vs. Amazon\", \"Microsoft vs. Google\",\n",
    "    \"Microsoft's market share\", \"Cloud market\",\n",
    "    \"PC market\", \"Gaming market\", \"Enterprise solutions\",\n",
    "    \"xbox vs playstation\", \"xbox vs ps4\", \"xbox vs ps2\",\n",
    "]\n",
    "\n",
    "microsoft_exclusion_keywords = [\n",
    "    # Unrelated \"windows\"\n",
    "    \"glass windows\", \"car windows\", \"house windows\", \"window cleaning\", \"window treatments\",\n",
    "    \n",
    "    # Unrelated \"surface\"\n",
    "    \"surface area\", \"surface water\", \"surface texture\", \"surface cleaning\", \"surface design\",\n",
    "\n",
    "    # Common phrases or idioms\n",
    "    \"opening a window of opportunity\", \"window into the soul\", \"surface-level examination\", \"only scratched the surface\",\n",
    "    \n",
    "    # Geographical references\n",
    "    \"Windows Valley\", \"Surface Creek\"\n",
    "]\n",
    "\n",
    "companies_keywords = {\n",
    "    'microsoft': {\n",
    "        'inclusion': microsoft_inclusion_keywords,\n",
    "        'exclusion': microsoft_exclusion_keywords\n",
    "    },\n",
    "    # Add more company queries with their keywords here\n",
    "}\n",
    "\n",
    "for company, keywords in companies_keywords.items():\n",
    "    get_articles_for(company, keywords['inclusion'], keywords['exclusion'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'file_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 55\u001b[0m\n\u001b[0;32m     40\u001b[0m microsoft_exclusion_keywords \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Unrelated \"windows\"\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mglass windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcar windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhouse windows\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow cleaning\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow treatments\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWindows Valley\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurface Creek\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m ]\n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Clean the dataset\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m cleaned_unfiltered_df \u001b[38;5;241m=\u001b[39m clean_dataset(\u001b[43mfile_path\u001b[49m, inclusion_keywords, exclusion_keywords)\n\u001b[0;32m     56\u001b[0m cleaned_unfiltered_df\n",
      "\u001b[1;31mNameError\u001b[0m: name 'file_path' is not defined"
     ]
    }
   ],
   "source": [
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "apple_inclusion_keywords = [\n",
    "    # Products and Hardware\n",
    "    \"iMac\", \"iMac Pro\",\n",
    "    \"Mac Mini\", \"Mac Pro\",\n",
    "    \"iPhone\", \"iPhone SE\", \"iPhone X\", \"iPhone 11\", \"iPhone 12\", \"iPhone 13\",\n",
    "    \"iPad Pro\", \"iPad Air\", \"iPad Mini\", \"iPadOS\",\n",
    "    \"Apple Watch Series\", \"watchOS\",\n",
    "    \"iPod\", \"iPod touch\", \"iPod Shuffle\",\n",
    "\n",
    "    # Software and Services\n",
    "    \"iOS updates\", \"macOS updates\", \"watchOS updates\",\n",
    "    \"iCloud\", \"iCloud Drive\", \"iCloud Photo Library\",\n",
    "    \"iTunes\", \"Apple Music\", \"Apple Podcasts\",\n",
    "    \"Apple TV+\", \"Apple TV app\", \"Apple Originals\",\n",
    "    \"Apple Arcade\", \"Game Center\", \"App Store\",\n",
    "\n",
    "    # Technology and Innovations\n",
    "    \"phones\", \"Retina display\", \"Liquid Retina\", \"Super Retina\",\n",
    "    \"Face ID\", \"Touch ID\", \"Apple Pay Cash\",\n",
    "    \"Apple M2 Chip\", \"A15 Bionic\", \"H1 Chip\",\n",
    "    \"TrueDepth camera\", \"Night mode\", \"Deep Fusion\",\n",
    "    \"LiDAR Scanner\", \"ARKit\",\n",
    "\n",
    "    # Corporate and Culture\n",
    "    \"Tim Cook\", \"Steve Jobs\", \"Steve Wozniak\", \"Jony Ive\",\n",
    "    \"Apple Store\", \"Genius Bar\", \"Today at Apple\",\n",
    "    \"Apple Campus 2\", \"Spaceship campus\",\n",
    "    \"Infinite Loop\", \"One Apple Park Way\",\n",
    "    \"Apple Design Awards\", \"Apple Entrepreneur Camp\",\n",
    "    \"AppleInsider\", \"MacRumors\",\n",
    "\n",
    "    # Events and Conferences\n",
    "    \"Special Events\", \"Apple Keynotes\",\n",
    "    \"Apple Spring Event\", \"Apple Fall Event\",\n",
    "    \"iPhone launch event\", \"iPad launch event\",\n",
    "    \"Apple Developer Forums\", \"Tech Talks\",\n",
    "    \"Apple Design Awards\", \"Shot on iPhone Challenge\",\n",
    "\n",
    "    # Competitors and Industry\n",
    "    \"Apple vs. Samsung\", \"Apple vs. Google\", \"Apple vs. Amazon\", \"Apple vs. Microsoft\",\n",
    "    \"iOS vs. Android\",\n",
    "    \"Mac vs. PC\",\n",
    "    \"Apple's market share\",\n",
    "    \"Innovations in consumer electronics\",\n",
    "    \"Trends in technology and mobile computing\",\n",
    "\n",
    "    # Potentially ambiguous headlines\n",
    "    \"Repurposing Your Dead Mac\",\n",
    "    \"Apple surprises us with a new, more-talkative iPod shuffle\",\n",
    "    \"Not Only Was Steve Jobs Sick, He Had A Liver Transplant\",\n",
    "    \"What's driving Steve Jobs?\",\n",
    "    \"A Suicide at an Apple Manufacturer in China\",\n",
    "    \"Compensation: $44000 And a MacBook - Cult of Mac\",\n",
    "    \"The iTunes App Store Rolls with the Travel Season\",\n",
    "    \"Steve Jobs Explains His Weight Loss in Healthnote\",\n",
    "    \"Apple iPod touch (3rd Generation) 32GB Review: iPod touch Review\",\n",
    "    \"He Put the Mac in Mackintosh\",\n",
    "    \"Apple's Iconic Ad\",\n",
    "    \"Apple's Latest Ad Takes Aim at Microsoft's 'Laptop Hunters' Campaign\",\n",
    "]\n",
    "\n",
    "apple_exclusion_keywords = [\n",
    "    # Fruit and agriculture\n",
    "    \"apple orchard\", \"apple harvest\", \"apple picking\", \"apple variety\", \"apple cider\", \"apples\",\n",
    "    \"apple pie\", \"apple crumble\", \"apple sauce\", \"apple dessert\", \"apple recipe\",\n",
    "    \"apple nutrition\", \"apple health benefits\", \"apple tree\", \"apple seed\", \"apple farming\",\n",
    "\n",
    "    # Geographical references\n",
    "    \"Big Apple\", \"Apple Hill\", \"Apple City\", \"Apple River\", \"Apple Blossom Festival\"\n",
    "    \n",
    "    # Common phrases or idioms\n",
    "    \"apple of my eye\", \"bad apple\", \"apple doesn't fall far from the tree\", \"upset the apple cart\", \"compare apples to oranges\",\n",
    "    \"eating healthy\", \n",
    "    \n",
    "    # Unrelated products or services\n",
    "    \"apple shampoo\", \"apple cosmetics\", \"apple fragrance\", \"apple scented\", \"apple flavor\",\n",
    "    \"apple soda\", \"apple juice\", \"apple wine\", \"apple brandy\", \"apple beer\",\n",
    "]\n",
    "\n",
    "# Extend the existing exclusion keywords list with the new ones\n",
    "apple_exclusion_keywords.extend([\n",
    "    \"Bikes With Non-EPA Tagged Exhausts Out of Big Apple\",\n",
    "    \"Copycat Fragrances\",\n",
    "    \"Swiss apple is key to Michelle Obama's youthful looks\",\n",
    "    \"Learn to grow tropical fruit at home in Houston\",\n",
    "    # Add any other previously identified exclusion keywords here...\n",
    "])\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_unfiltered_df = clean_dataset(file_path, inclusion_keywords, exclusion_keywords)\n",
    "cleaned_unfiltered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gn = GoogleNews()\n",
    "search = gn.search('apple', from_ = '2024-02-01', to_ = '2024-02-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to hold the article data\n",
    "articles = []\n",
    "\n",
    "# Iterate over the entries and extract the required information\n",
    "for entry in search['entries']:\n",
    "    article = {\n",
    "        'title': entry['title'],\n",
    "        'link': entry['link'],\n",
    "        'published': entry['published'],\n",
    "        'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "    }\n",
    "    articles.append(article)\n",
    "\n",
    "# Convert the list of articles into a DataFrame\n",
    "df = pd.DataFrame(articles)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
