{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News Article Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from pygooglenews import GoogleNews\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_articles(query, output_file_path, start_date='2009-01-01', end_date='2024-02-20'):\n",
    "    # Initialize GoogleNews\n",
    "    gn = GoogleNews()\n",
    "\n",
    "    # Check if the directory of the output file exists, create it if not\n",
    "    output_dir = os.path.dirname(output_file_path)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Convert string dates to datetime objects\n",
    "    start = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "    end = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "    # Initialize an empty list to hold the article data\n",
    "    articles = []\n",
    "    total_entries = 0\n",
    "\n",
    "    # Calculate the total number of days for the progress bar\n",
    "    total_days = (end - start).days + 1  # +1 to include the end date\n",
    "\n",
    "    # Track the last month processed\n",
    "    last_date = start\n",
    "\n",
    "    # Iterate through each day in the specified date range with tqdm\n",
    "    for day in tqdm(range(total_days), desc=f\"Collecting articles from {start_date} to {end_date}\"):\n",
    "        try:\n",
    "            current_date = start + timedelta(days=day)\n",
    "\n",
    "            # Check if the month has changed or it's the last day\n",
    "            if current_date.month != last_date.month or current_date == end:\n",
    "                if articles:\n",
    "                    # Convert the list of articles into a DataFrame and save\n",
    "                    df = pd.DataFrame(articles)\n",
    "                    file_exists = os.path.exists(output_file_path)\n",
    "                    df.to_csv(output_file_path, mode='a', index=False, header=not file_exists)\n",
    "\n",
    "                    # Reset articles list for the new month\n",
    "                    articles = []\n",
    "\n",
    "                last_date = current_date\n",
    "\n",
    "            # Format current_date for the search\n",
    "            search_date = current_date.strftime('%Y-%m-%d')\n",
    "            to_date = (current_date + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "\n",
    "            # Perform the search for the current day\n",
    "            search = gn.search(query, from_=search_date, to_=to_date)\n",
    "\n",
    "            # Iterate over the entries and extract the required information\n",
    "            for entry in search['entries']:\n",
    "                # Update total articles count\n",
    "                total_entries += 1\n",
    "                article = {\n",
    "                    'id': total_entries,\n",
    "                    'title': entry['title'],\n",
    "                    'link': entry['link'],\n",
    "                    'published': entry['published'],\n",
    "                    'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "                }\n",
    "                articles.append(article)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {current_date.strftime('%Y-%m-%d')}: {e}\")\n",
    "\n",
    "        # Save any remaining articles\n",
    "    if articles:\n",
    "        df = pd.DataFrame(articles).drop_duplicates()\n",
    "        file_exists = os.path.exists(output_file_path)\n",
    "        df.to_csv(output_file_path, mode='a', index=False, header=not file_exists)\n",
    "\n",
    "    # Return total articles saved\n",
    "    return total_entries\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning/ Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_file(df, base_name, cleaned_or_removed, filter_status):\n",
    "    \"\"\"\n",
    "    Saves the DataFrame to a CSV file with a structured naming convention.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame to be saved.\n",
    "    - base_name: The base name extracted from the original file path.\n",
    "    - cleaned_or_removed: Indicates whether the DataFrame contains cleaned or removed data.\n",
    "    - filter_status: Indicates the filtering status of the DataFrame ('unfiltered' or 'filtered').\n",
    "    \"\"\"\n",
    "    # Construct the directory path\n",
    "    dir_path = os.path.join(base_name, filter_status)\n",
    "    # Ensure the directory exists\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "    # Construct the full file path\n",
    "    file_name = f\"{base_name}_{cleaned_or_removed}_{filter_status}.csv\"\n",
    "    file_path = os.path.join(dir_path, file_name)\n",
    "\n",
    "    # Attempt to save the DataFrame, catching any exceptions\n",
    "    try:\n",
    "        df.to_csv(file_path, index=False)\n",
    "        print(f\"{cleaned_or_removed} dataset saved to {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save {cleaned_or_removed} dataset to {file_path}. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Articles By Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Check if a GPU is available and set the device accordingly\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def filter_titles_by_keywords(df, inclusion_keywords, exclusion_keywords, exclusion_included):\n",
    "    # Initialize tokenizer and model\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased').to(device)  # Move model to GPU if available\n",
    "\n",
    "    def get_embedding(text):\n",
    "        # Encode text and move tensors to the same device as model\n",
    "        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True).to(device)\n",
    "        # Perform the forward pass and don't compute gradients as we're not training\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1)\n",
    "\n",
    "    # Precompute keyword embeddings and move them to the device\n",
    "    inclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in inclusion_keywords]\n",
    "    if (exclusion_included):\n",
    "        exclusion_keyword_embeddings = [get_embedding(keyword).detach() for keyword in exclusion_keywords]\n",
    "\n",
    "    def is_related(title, positive_threshold=0.625, negative_threshold=0.73):\n",
    "        title_embedding = get_embedding(title).detach()\n",
    "        # Compute cosine similarities and check against thresholds\n",
    "        positive_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in inclusion_keyword_embeddings]\n",
    "        is_positive = any(similarity.item() > positive_threshold for similarity in positive_similarities)\n",
    "\n",
    "        if (exclusion_included):\n",
    "            negative_similarities = [torch.cosine_similarity(title_embedding, keyword_embedding) for keyword_embedding in exclusion_keyword_embeddings]\n",
    "            is_negative = any(similarity.item() > negative_threshold for similarity in negative_similarities)\n",
    "            return is_positive and not is_negative\n",
    "\n",
    "        return is_positive\n",
    "\n",
    "    # Apply the is_related function to each title\n",
    "    df['is_related'] = [is_related(title) for title in tqdm(df['title'], desc=\"Filtering Titles By Keywords\")]\n",
    "\n",
    "    # Filter the DataFrame to keep only related entries\n",
    "    related_df = df[df['is_related']].copy().drop(columns=['is_related'])\n",
    "\n",
    "    return related_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(file_path, inclusion_keywords, exclusion_keywords, exclusion_included):\n",
    "    # Load the dataset\n",
    "    original_df = pd.read_csv(file_path)\n",
    "    unfiltered_df = original_df.copy()\n",
    "\n",
    "    # Extract the base name of the file without extension\n",
    "    base_name = os.path.basename(file_path).split('.')[0]\n",
    "\n",
    "     # Remove duplicated data\n",
    "    unfiltered_df.drop_duplicates(subset=['title', 'link', 'published'], inplace=True)\n",
    "\n",
    "    # Remove entries with null values\n",
    "    unfiltered_df.dropna(inplace=True)\n",
    "\n",
    "    for col in unfiltered_df.select_dtypes(include='object').columns:\n",
    "        unfiltered_df[col] = unfiltered_df[col].str.replace('â€™', '\\'')\n",
    "        unfiltered_df[col] = unfiltered_df[col].str.replace('â€“', '-')\n",
    "\n",
    "    # Remove entries with non-ASCII characters\n",
    "    unfiltered_df = unfiltered_df[unfiltered_df['title'].map(lambda x: x.isascii()) & unfiltered_df['source'].map(lambda x: x.isascii())]\n",
    "\n",
    "    # Convert 'published' column to datetime to ensure proper sorting\n",
    "    unfiltered_df['published'] = pd.to_datetime(unfiltered_df['published'])\n",
    "\n",
    "    # Order the dataset by date\n",
    "    unfiltered_df.sort_values(by='published', inplace=True)\n",
    "\n",
    "    # Order the dataset by date\n",
    "    unfiltered_df.sort_values(by='published', inplace=True)\n",
    "\n",
    "    # Save the cleaned dataset before keyword filtering\n",
    "    save_file(unfiltered_df, base_name, 'cleaned', 'unfiltered')\n",
    "\n",
    "    # Identify and save the removed dataset before keyword filtering\n",
    "    removed_unfiltered_df = original_df[~original_df.index.isin(unfiltered_df.index)]\n",
    "    save_file(removed_unfiltered_df, base_name, 'removed', 'unfiltered')\n",
    "\n",
    "    # Filter non-company-related articles\n",
    "    filtered_df = filter_titles_by_keywords(unfiltered_df, inclusion_keywords, exclusion_keywords, exclusion_included)\n",
    "\n",
    "    # Save the cleaned and removed datasets after further processing\n",
    "    save_file(filtered_df, base_name, 'cleaned', 'filtered')\n",
    "    removed_filtered_df = unfiltered_df[~unfiltered_df.index.isin(filtered_df.index)]\n",
    "    save_file(removed_filtered_df, base_name, 'removed', 'filtered')\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Articles For"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_for(query, inclusion_keywords, exclusion_keywords, exclusion_included, start_date='2009-01-01', end_date='2024-02-20'):\n",
    "    # Path to raw dataset\n",
    "    file_path = f'./{query}/{query}.csv' \n",
    "    # # Collect the articles\n",
    "    num_data = collect_articles(query, file_path, start_date, end_date)\n",
    "    print(f\"Collected {num_data} articles and exported to '{file_path}'\")\n",
    "    # Clean the dataset\n",
    "    clean_dataset(file_path, inclusion_keywords, exclusion_keywords, exclusion_included)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "moderna_inclusion_keywords = [\n",
    "    # Vaccines and Medical Products\n",
    "    \"Moderna COVID-19 vaccine\", \"mRNA-1273\", \"Spikevax\",\n",
    "    \"Moderna booster shot\", \"Moderna vaccine efficacy\",\n",
    "    \n",
    "    # Research and Development\n",
    "    \"mRNA technology\", \"Moderna therapeutics\",\n",
    "    \"Moderna clinical trials\", \"Vaccine development\",\n",
    "    \"Moderna pipeline\", \"Moderna pharmaceuticals\",\n",
    "    \n",
    "    # Corporate Information\n",
    "    \"Moderna stock\", \"MRNA\", \"Moderna earnings\",\n",
    "    \"Moderna Inc. reports\", \"Moderna partnerships\",\n",
    "    \n",
    "    # Key Personnel\n",
    "    \"StÃ©phane Bancel\", \"Noubar Afeyan\", \"Tal Zaks\",\n",
    "    \"Stephen Hoge\", \"Moderna leadership\",\n",
    "    \n",
    "    # Technology and Innovations\n",
    "    \"Moderna mRNA platform\", \"Moderna research\",\n",
    "    \"Moderna patents\", \"Biotechnology innovation\",\n",
    "    \n",
    "    # Events and Conferences\n",
    "    \"Moderna presentations\", \"Biotech conferences\",\n",
    "    \"Moderna press release\", \"Pharmaceutical summits\",\n",
    "    \n",
    "    # Legal and Regulatory Issues\n",
    "    \"Moderna FDA approval\", \"Moderna vaccine authorization\",\n",
    "    \"Moderna regulatory submissions\",\n",
    "    \n",
    "    # Collaborations and Alliances\n",
    "    \"Moderna and NIH\", \"Moderna and BARDA\",\n",
    "    \"Moderna global distribution\", \"Vaccine alliances\",\n",
    "    \n",
    "    # Operations and Culture\n",
    "    \"Moderna headquarters\", \"Moderna manufacturing\",\n",
    "    \"Moderna corporate culture\", \"Moderna careers\",\n",
    "]\n",
    "\n",
    "moderna_exclusion_keywords = [\n",
    "    # Cultural and Artistic References\n",
    "    \"Museo Nacional Centro de Arte Reina SofÃ­a\", \"Museo Nacional Centro de Arte Reina SofÃ­a\",\n",
    "    \"Moderna Museet\",\n",
    "    \n",
    "    # General Modernity References\n",
    "    \"Modern art\", \"Modern architecture\",\n",
    "    \"Modern design\", \"Modern lifestyle\",\n",
    "    \n",
    "    # Unrelated Entities and Products\n",
    "    \"Moderna flooring\", \"Moderna products\",\n",
    "    \"Moderna housewares\", \"Moderna design services\",\n",
    "    \n",
    "    # Italian, Spanish, or Portuguese Language References\n",
    "    \"Cocina moderna\", \"modern kitchen\",\n",
    "    \"Vida moderna\", \"modern life\",\n",
    "    \"Arquitectura moderna\", \"modern architecture\",\n",
    "    \n",
    "    # Geographical Locations\n",
    "    \"Moderna, Italy\",\n",
    "    \n",
    "    # Ambiguous Phrases or Common Sayings\n",
    "    \"The modern age\", \"The modern era\",\n",
    "    \"Modern times\", \"Modern woman/man\",\n",
    "    \n",
    "    # Miscellaneous Unrelated References\n",
    "    \"Moderna dance\", \"modern dance\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Google...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20:   7%|â–‹         | 362/5529 [09:35<11:19:34,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 2009-12-28: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20:  18%|â–ˆâ–Š        | 1009/5529 [25:29<5:32:07,  4.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing 2011-10-06: ('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5529/5529 [2:05:04<00:00,  1.36s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 270141 articles and exported to './Google/Google.csv'\n",
      "cleaned dataset saved to Google\\unfiltered\\Google_cleaned_unfiltered.csv\n",
      "removed dataset saved to Google\\unfiltered\\Google_removed_unfiltered.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering Titles By Keywords: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 140714/140714 [2:49:06<00:00, 13.87it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned dataset saved to Google\\filtered\\Google_cleaned_filtered.csv\n",
      "removed dataset saved to Google\\filtered\\Google_removed_filtered.csv\n",
      "Working on Moderna...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting articles from 2009-01-01 to 2024-02-20:  21%|â–ˆâ–ˆ        | 1154/5529 [6:16:29<23:47:21, 19.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m company, keywords \u001b[38;5;129;01min\u001b[39;00m companies_keywords\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWorking on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m     \u001b[43mget_articles_for\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompany\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minclusion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexclusion\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeywords\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mexclusion_included\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[44], line 5\u001b[0m, in \u001b[0;36mget_articles_for\u001b[1;34m(query, inclusion_keywords, exclusion_keywords, exclusion_included, start_date, end_date)\u001b[0m\n\u001b[0;32m      3\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquery\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# # Collect the articles\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m num_data \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCollected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_data\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m articles and exported to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Clean the dataset\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[40], line 46\u001b[0m, in \u001b[0;36mcollect_articles\u001b[1;34m(query, output_file_path, start_date, end_date)\u001b[0m\n\u001b[0;32m     43\u001b[0m to_date \u001b[38;5;241m=\u001b[39m (current_date \u001b[38;5;241m+\u001b[39m timedelta(days\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Perform the search for the current day\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m search \u001b[38;5;241m=\u001b[39m \u001b[43mgn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msearch_date\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mto_date\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Iterate over the entries and extract the required information\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m search[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;66;03m# Update total articles count\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\Documents\\UNIVERSITY\\Year 3\\Project\\Produced Work\\MarketSenti_jupyter\\pygooglenews.py:154\u001b[0m, in \u001b[0;36mGoogleNews.search\u001b[1;34m(self, query, helper, when, from_, to_, proxies, scraping_bee)\u001b[0m\n\u001b[0;32m    151\u001b[0m search_ceid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__ceid()\n\u001b[0;32m    152\u001b[0m search_ceid \u001b[38;5;241m=\u001b[39m search_ceid\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m&\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 154\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__parse_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASE_URL\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/search?q=\u001b[39;49m\u001b[38;5;132;43;01m{}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msearch_ceid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscraping_bee\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscraping_bee\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    156\u001b[0m d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__add_sub_articles(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m d\n",
      "File \u001b[1;32mc:\\Users\\danie\\OneDrive\\Documents\\UNIVERSITY\\Year 3\\Project\\Produced Work\\MarketSenti_jupyter\\pygooglenews.py:81\u001b[0m, in \u001b[0;36mGoogleNews.__parse_feed\u001b[1;34m(self, feed_url, proxies, scraping_bee)\u001b[0m\n\u001b[0;32m     78\u001b[0m d \u001b[38;5;241m=\u001b[39m feedparser\u001b[38;5;241m.\u001b[39mparse(r\u001b[38;5;241m.\u001b[39mtext)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m scraping_bee \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m proxies \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(d[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 81\u001b[0m     d \u001b[38;5;241m=\u001b[39m \u001b[43mfeedparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeed_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mdict\u001b[39m((k, d[k]) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeed\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentries\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\feedparser\\api.py:216\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, response_headers, resolve_relative_uris, sanitize_html)\u001b[0m\n\u001b[0;32m    208\u001b[0m result \u001b[38;5;241m=\u001b[39m FeedParserDict(\n\u001b[0;32m    209\u001b[0m     bozo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    210\u001b[0m     entries\u001b[38;5;241m=\u001b[39m[],\n\u001b[0;32m    211\u001b[0m     feed\u001b[38;5;241m=\u001b[39mFeedParserDict(),\n\u001b[0;32m    212\u001b[0m     headers\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    213\u001b[0m )\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 216\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_open_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_file_stream_or_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandlers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mURLError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[0;32m    218\u001b[0m     result\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbozo\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbozo_exception\u001b[39m\u001b[38;5;124m'\u001b[39m: error,\n\u001b[0;32m    221\u001b[0m     })\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\feedparser\\api.py:115\u001b[0m, in \u001b[0;36m_open_resource\u001b[1;34m(url_file_stream_or_string, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m url_file_stream_or_string\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(url_file_stream_or_string, \u001b[38;5;28mstr\u001b[39m) \\\n\u001b[0;32m    114\u001b[0m    \u001b[38;5;129;01mand\u001b[39;00m urllib\u001b[38;5;241m.\u001b[39mparse\u001b[38;5;241m.\u001b[39murlparse(url_file_stream_or_string)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mftp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeed\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_file_stream_or_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodified\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferrer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandlers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;66;03m# try to open with native open function (if url_file_stream_or_string is a filename)\u001b[39;00m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\feedparser\\http.py:171\u001b[0m, in \u001b[0;36mget\u001b[1;34m(url, etag, modified, agent, referrer, handlers, request_headers, result)\u001b[0m\n\u001b[0;32m    169\u001b[0m opener \u001b[38;5;241m=\u001b[39m urllib\u001b[38;5;241m.\u001b[39mrequest\u001b[38;5;241m.\u001b[39mbuild_opener(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mtuple\u001b[39m(handlers \u001b[38;5;241m+\u001b[39m [_FeedURLHandler()]))\n\u001b[0;32m    170\u001b[0m opener\u001b[38;5;241m.\u001b[39maddheaders \u001b[38;5;241m=\u001b[39m []  \u001b[38;5;66;03m# RMK - must clear so we only send our custom User-Agent\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[43mopener\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m data \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    173\u001b[0m f\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py:519\u001b[0m, in \u001b[0;36mOpenerDirector.open\u001b[1;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[0;32m    516\u001b[0m     req \u001b[38;5;241m=\u001b[39m meth(req)\n\u001b[0;32m    518\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124murllib.Request\u001b[39m\u001b[38;5;124m'\u001b[39m, req\u001b[38;5;241m.\u001b[39mfull_url, req\u001b[38;5;241m.\u001b[39mdata, req\u001b[38;5;241m.\u001b[39mheaders, req\u001b[38;5;241m.\u001b[39mget_method())\n\u001b[1;32m--> 519\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[38;5;66;03m# post-process response\u001b[39;00m\n\u001b[0;32m    522\u001b[0m meth_name \u001b[38;5;241m=\u001b[39m protocol\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_response\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py:536\u001b[0m, in \u001b[0;36mOpenerDirector._open\u001b[1;34m(self, req, data)\u001b[0m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m    535\u001b[0m protocol \u001b[38;5;241m=\u001b[39m req\u001b[38;5;241m.\u001b[39mtype\n\u001b[1;32m--> 536\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_open\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[0;32m    537\u001b[0m \u001b[43m                          \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m_open\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result:\n\u001b[0;32m    539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py:496\u001b[0m, in \u001b[0;36mOpenerDirector._call_chain\u001b[1;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers:\n\u001b[0;32m    495\u001b[0m     func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(handler, meth_name)\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    497\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    498\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py:1391\u001b[0m, in \u001b[0;36mHTTPSHandler.https_open\u001b[1;34m(self, req)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhttps_open\u001b[39m(\u001b[38;5;28mself\u001b[39m, req):\n\u001b[1;32m-> 1391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhttp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHTTPSConnection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_hostname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_hostname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\urllib\\request.py:1352\u001b[0m, in \u001b[0;36mAbstractHTTPHandler.do_open\u001b[1;34m(self, http_class, req, **http_conn_args)\u001b[0m\n\u001b[0;32m   1350\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err: \u001b[38;5;66;03m# timeout error\u001b[39;00m\n\u001b[0;32m   1351\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m URLError(err)\n\u001b[1;32m-> 1352\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mh\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetresponse\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1353\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1354\u001b[0m     h\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:1390\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1388\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1389\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1390\u001b[0m         \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbegin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1391\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m:\n\u001b[0;32m   1392\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:325\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    323\u001b[0m \u001b[38;5;66;03m# read until we get a non-100 response\u001b[39;00m\n\u001b[0;32m    324\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m--> 325\u001b[0m     version, status, reason \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;241m!=\u001b[39m CONTINUE:\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\http\\client.py:286\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_read_status\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 286\u001b[0m     line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp\u001b[38;5;241m.\u001b[39mreadline(_MAXLINE \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miso-8859-1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) \u001b[38;5;241m>\u001b[39m _MAXLINE:\n\u001b[0;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus line\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[0;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1314\u001b[0m, in \u001b[0;36mSSLSocket.recv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1311\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1312\u001b[0m           \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m   1313\u001b[0m           \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[1;32m-> 1314\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv_into(buffer, nbytes, flags)\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_3.11.2288.0_x64__qbz5n2kfra8p0\\Lib\\ssl.py:1166\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m   1164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "\n",
    "start_date='2022-02-01'\n",
    "end_date='2013-01-10'\n",
    "\n",
    "companies_keywords = {\n",
    "    'Moderna': {\n",
    "        'inclusion': moderna_inclusion_keywords,\n",
    "        'exclusion': moderna_exclusion_keywords,\n",
    "        'exclusion_included': True,\n",
    "    },\n",
    "    # Add more company queries with their keywords here\n",
    "}\n",
    "\n",
    "for company, keywords in companies_keywords.items():\n",
    "    print(f\"Working on {company}...\")\n",
    "    get_articles_for(company, keywords['inclusion'], keywords['exclusion'], keywords['exclusion_included'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keywords to compute keyword embeddings and compare with article tites\n",
    "apple_inclusion_keywords = [\n",
    "    # Products and Hardware\n",
    "    \"iMac\", \"iMac Pro\",\n",
    "    \"Mac Mini\", \"Mac Pro\",\n",
    "    \"iPhone\", \"iPhone SE\", \"iPhone X\", \"iPhone 11\", \"iPhone 12\", \"iPhone 13\",\n",
    "    \"iPad Pro\", \"iPad Air\", \"iPad Mini\", \"iPadOS\",\n",
    "    \"Apple Watch Series\", \"watchOS\",\n",
    "    \"iPod\", \"iPod touch\", \"iPod Shuffle\",\n",
    "\n",
    "    # Software and Services\n",
    "    \"iOS updates\", \"macOS updates\", \"watchOS updates\",\n",
    "    \"iCloud\", \"iCloud Drive\", \"iCloud Photo Library\",\n",
    "    \"iTunes\", \"Apple Music\", \"Apple Podcasts\",\n",
    "    \"Apple TV+\", \"Apple TV app\", \"Apple Originals\",\n",
    "    \"Apple Arcade\", \"Game Center\", \"App Store\",\n",
    "\n",
    "    # Technology and Innovations\n",
    "    \"phones\", \"Retina display\", \"Liquid Retina\", \"Super Retina\",\n",
    "    \"Face ID\", \"Touch ID\", \"Apple Pay Cash\",\n",
    "    \"Apple M2 Chip\", \"A15 Bionic\", \"H1 Chip\",\n",
    "    \"TrueDepth camera\", \"Night mode\", \"Deep Fusion\",\n",
    "    \"LiDAR Scanner\", \"ARKit\",\n",
    "\n",
    "    # Corporate and Culture\n",
    "    \"Tim Cook\", \"Steve Jobs\", \"Steve Wozniak\", \"Jony Ive\",\n",
    "    \"Apple Store\", \"Genius Bar\", \"Today at Apple\",\n",
    "    \"Apple Campus 2\", \"Spaceship campus\",\n",
    "    \"Infinite Loop\", \"One Apple Park Way\",\n",
    "    \"Apple Design Awards\", \"Apple Entrepreneur Camp\",\n",
    "    \"AppleInsider\", \"MacRumors\",\n",
    "\n",
    "    # Events and Conferences\n",
    "    \"Special Events\", \"Apple Keynotes\",\n",
    "    \"Apple Spring Event\", \"Apple Fall Event\",\n",
    "    \"iPhone launch event\", \"iPad launch event\",\n",
    "    \"Apple Developer Forums\", \"Tech Talks\",\n",
    "    \"Apple Design Awards\", \"Shot on iPhone Challenge\",\n",
    "\n",
    "    # Competitors and Industry\n",
    "    \"Apple vs. Samsung\", \"Apple vs. Google\", \"Apple vs. Amazon\", \"Apple vs. Microsoft\",\n",
    "    \"iOS vs. Android\",\n",
    "    \"Mac vs. PC\",\n",
    "    \"Apple's market share\",\n",
    "    \"Innovations in consumer electronics\",\n",
    "    \"Trends in technology and mobile computing\",\n",
    "\n",
    "    # Potentially ambiguous headlines\n",
    "    \"Repurposing Your Dead Mac\",\n",
    "    \"Apple surprises us with a new, more-talkative iPod shuffle\",\n",
    "    \"Not Only Was Steve Jobs Sick, He Had A Liver Transplant\",\n",
    "    \"What's driving Steve Jobs?\",\n",
    "    \"A Suicide at an Apple Manufacturer in China\",\n",
    "    \"Compensation: $44000 And a MacBook - Cult of Mac\",\n",
    "    \"The iTunes App Store Rolls with the Travel Season\",\n",
    "    \"Steve Jobs Explains His Weight Loss in Healthnote\",\n",
    "    \"Apple iPod touch (3rd Generation) 32GB Review: iPod touch Review\",\n",
    "    \"He Put the Mac in Mackintosh\",\n",
    "    \"Apple's Iconic Ad\",\n",
    "    \"Apple's Latest Ad Takes Aim at Microsoft's 'Laptop Hunters' Campaign\",\n",
    "]\n",
    "\n",
    "apple_exclusion_keywords = [\n",
    "    # Fruit and agriculture\n",
    "    \"apple orchard\", \"apple harvest\", \"apple picking\", \"apple variety\", \"apple cider\", \"apples\",\n",
    "    \"apple pie\", \"apple crumble\", \"apple sauce\", \"apple dessert\", \"apple recipe\",\n",
    "    \"apple nutrition\", \"apple health benefits\", \"apple tree\", \"apple seed\", \"apple farming\",\n",
    "\n",
    "    # Geographical references\n",
    "    \"Big Apple\", \"Apple Hill\", \"Apple City\", \"Apple River\", \"Apple Blossom Festival\"\n",
    "    \n",
    "    # Common phrases or idioms\n",
    "    \"apple of my eye\", \"bad apple\", \"apple doesn't fall far from the tree\", \"upset the apple cart\", \"compare apples to oranges\",\n",
    "    \"eating healthy\", \n",
    "    \n",
    "    # Unrelated products or services\n",
    "    \"apple shampoo\", \"apple cosmetics\", \"apple fragrance\", \"apple scented\", \"apple flavor\",\n",
    "    \"apple soda\", \"apple juice\", \"apple wine\", \"apple brandy\", \"apple beer\",\n",
    "]\n",
    "\n",
    "# Extend the existing exclusion keywords list with the new ones\n",
    "apple_exclusion_keywords.extend([\n",
    "    \"Bikes With Non-EPA Tagged Exhausts Out of Big Apple\",\n",
    "    \"Copycat Fragrances\",\n",
    "    \"Swiss apple is key to Michelle Obama's youthful looks\",\n",
    "    \"Learn to grow tropical fruit at home in Houston\",\n",
    "    # Add any other previously identified exclusion keywords here...\n",
    "])\n",
    "\n",
    "# Clean the dataset\n",
    "cleaned_unfiltered_df = clean_dataset(file_path, inclusion_keywords, exclusion_keywords)\n",
    "cleaned_unfiltered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gn = GoogleNews()\n",
    "# search = gn.search('apple', from_ = '2024-02-01', to_ = '2024-02-20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize an empty list to hold the article data\n",
    "# articles = []\n",
    "\n",
    "# # Iterate over the entries and extract the required information\n",
    "# for entry in search['entries']:\n",
    "#     article = {\n",
    "#         'title': entry['title'],\n",
    "#         'link': entry['link'],\n",
    "#         'published': entry['published'],\n",
    "#         'source': entry['source']['title'] if 'source' in entry else 'Unknown'\n",
    "#     }\n",
    "#     articles.append(article)\n",
    "\n",
    "# # Convert the list of articles into a DataFrame\n",
    "# df = pd.DataFrame(articles)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = f'./Apple/apple.csv' \n",
    "\n",
    "# year = 2018\n",
    "\n",
    "# month = 4\n",
    "\n",
    "# # Load the CSV file into a DataFrame\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "# # Convert the date column to datetime format\n",
    "# # Replace 'published' with the name of your date column\n",
    "# df['published'] = pd.to_datetime(df['published'])\n",
    "\n",
    "# # Filter the DataFrame for the desired year\n",
    "# # Replace 2020 with the year you want to extract\n",
    "# df_year = df[(df['published'].dt.year == year) & (df['published'].dt.month == month)]\n",
    "\n",
    "# # Save the filtered DataFrame to a new CSV file\n",
    "# df_year.to_csv(f'./test_data/raw/apple_{year}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def merge_datasets(file_path1, file_path2, output_file_path):\n",
    "#     # Load the datasets from the given file paths\n",
    "#     df1 = pd.read_csv(file_path1)\n",
    "#     df2 = pd.read_csv(file_path2)\n",
    "    \n",
    "#     # Assuming 'id' is the column name for IDs in your datasets\n",
    "#     # Adjust IDs in df2 to continue from the last ID in df1\n",
    "#     last_id_df1 = df1['id'].max()\n",
    "#     df2['id'] = df2['id'].apply(lambda x: x + last_id_df1)\n",
    "\n",
    "#     # Merge the datasets without resetting the index\n",
    "#     merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "    \n",
    "#     # Save the resulting dataset to the specified file\n",
    "#     merged_df.to_csv(output_file_path, index=False)\n",
    "\n",
    "#     print(f\"Merged dataset saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_dates(file_path, date_to_remove):\n",
    "\n",
    "#     df = pd.read_csv(file_path)\n",
    "\n",
    "#     df['published'] = pd.to_datetime(df['published'])\n",
    "\n",
    "#     # Convert the string to a datetime object for comparison\n",
    "#     date_to_remove = pd.to_datetime(date_to_remove)\n",
    "\n",
    "#     # Filter out the entries for the specified date\n",
    "#     df_filtered = df[df['published'].dt.date != date_to_remove.date()]\n",
    "\n",
    "#     # Save the resulting dataset to the specified file\n",
    "#     df_filtered.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/Amazon/filtered/Amazon_removed_filtered.csv')\n",
    "df = df.drop(['is_related'], axis=1)\n",
    "df.to_csv('./data/Amazon/filtered/Amazon_removed_filtered_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def count_entries_for_date(df, date_column, date_to_count):\n",
    "#     # Convert the date column to datetime format\n",
    "#     df[date_column] = pd.to_datetime(df[date_column])\n",
    "\n",
    "#     # Create a boolean mask for rows that match the given date\n",
    "#     mask = df[date_column].dt.date == pd.to_datetime(date_to_count).date()\n",
    "    \n",
    "#     # Count the number of entries for the given date\n",
    "#     count = mask.sum()\n",
    "    \n",
    "#     return count\n",
    "\n",
    "# # Usage example\n",
    "# df = pd.read_csv('./Nvidia/Nvidia.csv')  # Replace with your DataFrame loading method\n",
    "# start_date = '2013-01-01'  # Replace with the date you want to count entries for\n",
    "# end_date = '2013-01-10'  # Replace with the date you want to count entries for\n",
    "\n",
    "# # Convert start and end dates to datetime objects\n",
    "# start_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "# end_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "\n",
    "# date_to_count = start_date\n",
    "# while date_to_count <= end_date:\n",
    "#     number_of_entries = count_entries_for_date(df, 'published', date_to_count)\n",
    "#     print(f\"Number of entries for {date_to_count}: {number_of_entries}\")\n",
    "#     date_to_count += timedelta(days=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
